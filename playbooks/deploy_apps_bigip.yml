---
# This playbook deploys virtual severs and all related BIG-IP and EC2 resources. 
 
###########################################################################################
###### Our typical (nasty) boilerplate to handle variable persistence within ansible. ######

# Add CFT output variables to host from persisted results from previous playbooks
- hosts: bigips
  gather_facts: no
  connection: local
  vars_files:
      - [ "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}.yml" ]
  pre_tasks:
    - name: Add CFT output variables to host from persisted results from previous playbooks
      set_fact:
        ansible_ssh_host={{ hostvars[inventory_hostname].stack_outputs.ManagementInterfacePublicIp }}
        ManagementInterfacePublicIp={{ hostvars[inventory_hostname].stack_outputs.ManagementInterfacePublicIp }}
        ManagementInterfacePrivateIp={{ hostvars[inventory_hostname].stack_outputs.ManagementInterfacePrivateIp }}
        ExternalInterfacePublicIp={{ hostvars[inventory_hostname].stack_outputs.ExternalInterfacePublicIp }}
        ExternalInterfacePrivateIp={{ hostvars[inventory_hostname].stack_outputs.ExternalInterfacePrivateIp }}
        InternalInterfacePrivateIp={{ hostvars[inventory_hostname].stack_outputs.InternalInterfacePrivateIp }}
        VipAddress1={{ hostvars[inventory_hostname].stack_outputs.Vip1 }}
        VipAddress2={{ hostvars[inventory_hostname].stack_outputs.Vip2 }}
        DeviceName='ip-{{hostvars[inventory_hostname].stack_outputs.ManagementInterfacePrivateIp|replace(".","-")}}.{{region}}.ec2.internal'

- hosts: bigip-clusters
  gather_facts: no
  connection: local
  tasks:
    - name: re-create a dynamic group of seed devices using first member of each group
      add_host:
          name: "{{ item.value.0 }}"
          group: bigip-cluster-seeds
          cluster_name: "{{ item.key }}"
          members: "{{ item.value }}"
      with_dict: groups
      when: item.key in groups['bigip-clusters']

- hosts: apphosts
  gather_facts: no
  vars_files:
   - [ "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}.yml" ]
   - [ "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}_docker_containers.yml" ]
  tasks:
    - name: Add CFT output variables to host from persisted results from previous playbooks
      set_fact:
        ansible_ssh_host={{ hostvars[inventory_hostname].stack_outputs.WebServerInstancePublicIp }}
        WebServerInstancePublicIp={{ hostvars[inventory_hostname].stack_outputs.WebServerInstancePublicIp }}
        WebServerInstancePrivateIp={{ hostvars[inventory_hostname].stack_outputs.WebServerInstancePrivateIp }}

############################## End boilerplate #########################
########################################################################
# At this point we have dynamically populated the ansible host groups,
#  variables, and execution data from previous playbooks
#  that are necessary to begin provisioning our application resources...
# 
# We will execute most of the following provisioning steps against hosts
#  in the 'bigip-cluster-seeds' group.  In this way, we are provisioning 
#  against only a single device in the cluster. The config-sync feature
#  will handle configuration of secondary devices in Device Service
#  Clusters as necessary. Remember that a cluster may only include 1
#  device, in which case that will be active. 


######################################################################################
######################################## app 1 ####################################
# We leverage iApps as a way to consistently deploy the TMOS traffic configuration.
#  iApps are F5â€™s powerful re-entrant templates for creating virtual services. They
#  allow you to see and manage all the elements for the virtual service while
#  providing custom menus using language that all users in your organization can understand.
#  This built-in iApp has a few fun iRules attached (one that posts an Sorry Page
#  if the virtual service is down and another that sends log data to a remote log
#  server for additional Analytics/Reporting)
#  A template of the iApp we deploy is available in:
#   /roles/bigip_app/templates/demo_iApp.cfg.j2.
# The playbook also deploys the supporting content for the iApp:
#  1) iRules including a few demonstrating analytics integration, and displaying sorry page. 
#     These iRules can be seen in ./roles/bigip_app/files/
#  2) Background and sorry page images to an internal data-group. 
#      These images can be found in ./roles/bigip_app/files/
# After deploying the iApp which contains the virtual server, we attach an EIP to the
#  secondary IP address in Amazon.  This secondary EIP matches the VIP. 

# Prepare the json payload for the ltm pools based on output from container
#    provisioning tasks.  Use jinja templates for this. 
# We strip out last comma from jinja template output 
#    json_payload: "{{json_output|regex_replace(',]}', ']}' ) }}"
#    easier for now to modify in place with ansible replace command than jinja regex filter
- hosts: apphosts
  gather_facts: no
  vars:
    vip_id:  "Vip1"
  tasks:
    - name: Store pool members from containers into json
      local_action: template src=../roles/bigip_app1/templates/bigip_pool_members_from_containers.cfg.j2 dest=~/vars/f5aws/env/{{ env_name }}/{{vip_id}}_pool_from_containers.json
    - replace: dest=~/vars/f5aws/env/{{ env_name }}/{{vip_id}}_pool_from_containers.json regexp=',]}' replace=']}'
      delegate_to: localhost

# Deploy virtual server and related resources to BIG-IP 
- hosts: bigip-cluster-seeds
  gather_facts: no
  vars:
    vip_id: "Vip1"
    # lookup static content our iapp will reference, pass as variables to the role
    image_background: "{{ lookup('file', install_path + '/roles/bigip_app1/files/image_background_base_64') }}"
    image_sorry: "{{ lookup('file', install_path + '/roles/bigip_app1/files/image_sorry_base_64') }}"
    irule_sorry_page: "{{ lookup('file', install_path + '/roles/bigip_app1/files/irule_sorry_page.tcl') }}"
    irule_demo_analytics: "{{ lookup('file', install_path + '/roles/bigip_app1/files/irule_demo_analytics.tcl') }}"
    asm_policy_linux_high_base64: "{{ lookup('file', install_path + '/roles/bigip_app1/files/asm_policy_linux_high_base64') }}"
    asm_policy_name: "linux_high-{{ vip_id }}"
    ltm_policy_name: "ltm_policy_w_asm_linux_high-{{ vip_id }}"
  roles:
    - bigip_app1

# Finally attach an EIP to the VIP / Application. One per cluster
- hosts: bigip-cluster-seeds
  gather_facts: no
  vars:
     ansible_connection: local
     ansible_python_interpreter: "/usr/bin/env python"
     vip_id: "Vip1"
     owner: "app1"
  tasks:
    - name: deploy eips
      include: "{{ install_path }}/roles/infra/tasks/deploy_eip_cft.yml"
      delegate_to: localhost


######################################################################################
######################################## app 2 ####################################

# For this second virtual, we do not use iApps.  Instead, we directly use the
#  bigip_config ansible module we have written. The module is
#  used to deploy an iRule, virtual servers for 80 and 443, and an web server pool.
# The iRule we deploy will randomly SNAT and redirect traffic to the first virtual
#  server mentioned above. This helps to provide some interesting
#  graphs for viewing traffic in the Analytics module or elsewhere. 
# Finally, for this virtual, we do not attach an EIP as we did previously. The virtual 
#  server will not be reachable from the public internet, but only within the VPC. 
#  This step is skipped in order to save on Elastic IP addresses, which are in short supply.

# Prepare the json payload for the ltm pools based on output from container
#    provisioning tasks.  Use jinja templates for this. 
- hosts: apphosts
  gather_facts: no
  vars:
    vip_id:  "Vip2"
  tasks:
    - name: Create a json payload for the pool members
      local_action: template src=../roles/bigip_app1/templates/bigip_pool_members_from_containers.cfg.j2 dest=~/vars/f5aws/env/{{ env_name }}/{{vip_id}}_pool_from_containers.json
    - replace: dest=~/vars/f5aws/env/{{ env_name }}/{{vip_id}}_pool_from_containers.json regexp=',]}' replace=']}'
      delegate_to: localhost

# Deploy virtual server and related resources to BIG-IP 
- hosts: bigip-cluster-seeds
  gather_facts: no
  vars:
    vip_id: "Vip2"
    irule_random_snat: "{{ lookup('file', install_path + '/roles/bigip_app2/files/irule_random_snat.tcl') }}"
  roles:
    - bigip_app2

##### Done with application deployment, retrieve some information from the device #######
####################################################################################
- hosts: bigips
  gather_facts: no
  vars_files:
  - [ "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}.yml" ]
  tasks:
  - name: Add CFT output variables to host from persisted results from previous playbooks
    set_fact: ansible_ssh_host={{ hostvars[inventory_hostname].stack_outputs.ManagementInterfacePublicIp }}
  - name: Get ltm virtual information
    delegate_to: localhost
    bigip_config:
       state=inspect
       host={{ ansible_ssh_host }}
       user={{ bigip_rest_user }}
       password={{ bigip_rest_password }}
       collection_path='mgmt/tm/ltm/virtual'
    register: result

  - copy: content="{{ result['out'] | to_json }}" dest=~/vars/f5aws/env/{{ env_name }}/facts_{{ inventory_hostname }}.json
    delegate_to: localhost


