---
# This playbook deploys a virtual server and all related BIG-IP and EC2 resources. 
# We leverage iApps as a way to consistently deploy the TMOS traffic configuration.
#  iApps are F5â€™s powerful re-entrant templates for creating virtual services. They
#  allow you to see and manage all the elements for the virtual service while
#  providing custom menus using language that all users in your organization can understand.
#  This built-in iApp has a few fun iRules attached (one that posts an Sorry Page
#  if the virtual service is down and another that sends log data to a remote log
#  server for additional Analytics/Reporting)
#  A template of the iApp we deploy is available in:
#   /roles/bigip_app/templates/demo_iApp.cfg.j2.
# The playbook also deploys the supporting content for the iApp:
#  1) iRules including a few demonstrating analytics integration, and displaying sorry page. 
#     These iRules can be seen in ./roles/bigip_app/files/
#  2) Background and sorry page images to an internal data-group. 
#      These images can be found in ./roles/bigip_app/files/
# After deploying the iApp which contains the virtual server, we attach an EIP to the
#  secondary IP address in Amazon.  This secondary EIP matches the VIP. 


#####################################################################################
###### Our typical boilerplate to handle variable persistence within ansible. ######

# Set ansible facts for the bigips group from previous playbook executions
- hosts: bigips
  gather_facts: no
  connection: local
  vars_files:
      - [ "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}.yml" ]
  pre_tasks:
    - name: Add CFT output variables to host from persisted results from previous playbooks
      set_fact:
        ansible_ssh_host={{ hostvars[inventory_hostname].stack_outputs.ManagementInterfacePublicIp }}
        ManagementInterfacePublicIp={{ hostvars[inventory_hostname].stack_outputs.ManagementInterfacePublicIp }}
        ManagementInterfacePrivateIp={{ hostvars[inventory_hostname].stack_outputs.ManagementInterfacePrivateIp }}
        ExternalInterfacePublicIp={{ hostvars[inventory_hostname].stack_outputs.ExternalInterfacePublicIp }}
        ExternalInterfacePrivateIp={{ hostvars[inventory_hostname].stack_outputs.ExternalInterfacePrivateIp }}
        InternalInterfacePrivateIp={{ hostvars[inventory_hostname].stack_outputs.InternalInterfacePrivateIp }}
        VipAddress={{ hostvars[inventory_hostname].stack_outputs.Vip1 }}
        DeviceName='ip-{{hostvars[inventory_hostname].stack_outputs.ManagementInterfacePrivateIp|replace(".","-")}}.{{region}}.ec2.internal'

# Re-populate the bigip-clusters host group
- hosts: bigip-clusters
  gather_facts: no
  connection: local
  tasks:
    - name: re-create a dynamic group of seed devices using first member of each group
      add_host:
          name: "{{ item.value.0 }}"
          group: bigip-cluster-seeds
          cluster_name: "{{ item.key }}"
          members: "{{ item.value }}"
      with_dict: groups
      when: item.key in groups['bigip-clusters']

# Set ansible facts for the apphosts group from previous playbook executions
- hosts: apphosts
  gather_facts: no
  vars_files:
   - [ "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}.yml" ]
   - [ "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}_docker_containers.yml" ]
  tasks:
    - name: Add CFT output variables to host from persisted results from previous playbooks
      set_fact:
        ansible_ssh_host={{ hostvars[inventory_hostname].stack_outputs.WebServerInstancePublicIp }}
        WebServerInstancePublicIp={{ hostvars[inventory_hostname].stack_outputs.WebServerInstancePublicIp }}
        WebServerInstancePrivateIp={{ hostvars[inventory_hostname].stack_outputs.WebServerInstancePrivateIp }}

# Extract information about our docker containers (ip, port) from previous playbook executions.
- hosts: apphosts
  gather_facts: no
  vars:
    vip_id: "Vip1"
  tasks:
    - name: Store pool members from containers into json
      local_action: template src=../roles/bigip_app/templates/bigip_pool_members_from_containers_test.cfg.j2 dest=~/vars/f5aws/env/{{ env_name }}/{{vip_id}}_pool_from_containers.json

     # Notes:
     # strip out last comma from jinja template output (see note below)
     # json.loads is more sensitive than our parser and need to strip out last "," at the end of the pool member list
     # no clean way to use loop.last directive in jinja template because it's a nested loop over all docker hosts
     # Could try a inline replacement
     # json_payload: "{{json_output|regex_replace(',]}', ']}' ) }}"
     # easier for now to modify in place with ansible replace command than jinja regex filter

    - replace: dest=~/vars/f5aws/env/{{ env_name }}/{{vip_id}}_pool_from_containers.json regexp=',]}' replace=']}'
      delegate_to: localhost

########################################################################
############################## End boilerplate #########################

# At this point we have dynamically populated the ansible host groups,
#  variables, and execution data from previous playbooks
#  that are necessary to deploy our application resources...
# 
# We will execute most of the following provisioning steps against hosts
#  in the 'bigip-cluster-seeds' group.  In this way, we are provisioning 
#  against only a single device in the cluster. The config-sync feature
#  will handle configuration of secondary devices in Device Service
#  Clusters as necessary. 

# Generate the iApp we will deploy from a template (this is a local action)
- hosts: bigip-cluster-seeds
  gather_facts: no
  connection: local
  vars:
    vip_id: "Vip1"
  vars_files:
      - [ "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}.yml" ]
  tasks: 
     - name: Modify iApp to use VipAddress
       template: src='{{ install_path }}/roles/bigip_app/templates/demo_iApp.cfg.j2' dest='~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}-{{vip_id}}-iApp.yml'

# Deploy virtual server and related resources to BIG-IP 
- hosts: bigip-cluster-seeds
  gather_facts: no
  vars:
    vip_id: "Vip1"

    # Ansible modules (lookup/cat/etc.) auto converts file contents that contain json format into python dicts 
    # so have to remove new lines and convert back to json ( with to_nice_json filter) so json modules don't fail 
    # ex. 
    # json/decoder.py", line 381, in raw_decode 
    background_image: "{{ lookup('file', install_path + '/roles/bigip_app/files/background_image_base_64') }}"
    sorry_image: "{{ lookup('file', install_path + '/roles/bigip_app/files/sorry_image_base_64') }}"
    sorry_page_rule: "{{ lookup('file', install_path + '/roles/bigip_app/files/__sorry_page_rule.tcl') }}"
    demo_analytics_rule: "{{ lookup('file', install_path + '/roles/bigip_app/files/__demo_analytics_rule.tcl') }}"

  vars_files:
      - [ "~/vars/f5aws/env/{{env_name}}/{{inventory_hostname}}-{{vip_id}}-iApp.yml" ]
  tasks:
    - name: Deploying/updating Webserver Pool
      delegate_to: localhost
      bigip_config:
          state=present
          host={{ ansible_ssh_host }}
          user={{ bigip_rest_user }}
          password={{ bigip_rest_password }}
          payload='{{lookup('file', '~/vars/f5aws/env/' + env_name + '/' + vip_id + '_pool_from_containers.json')}}'
          collection_path='mgmt/tm/ltm/pool'
          resource_key="name"

    - name: Deploying/updating High Speed Logging pool to send to Analytics Server
      delegate_to: localhost
      bigip_config:
          state=present
          host={{ ansible_ssh_host }}
          user={{ bigip_rest_user }}
          password={{ bigip_rest_password }}
          collection_path='mgmt/tm/ltm/pool'
          resource_key="name"
          payload='{"name":"syslog_pool","members":[{"name":"10.0.3.32:514","address":"10.0.3.32"},{"name":"10.0.3.33:514","address":"10.0.3.33"}],"monitor":"tcp"}' 
    
    ##### UPLOAD DATAGROUP #####
    # TODO:
    # "Setting SSL Profiles"
    # "Setting Remote Logging Profiles"
 
    - name: Deploying/updating Analytics Profile
      delegate_to: localhost
      bigip_config:
          state=present
          host={{ ansible_ssh_host }}
          user={{ bigip_rest_user }}
          password={{ bigip_rest_password }}
          collection_path='mgmt/tm/ltm/profile/analytics'
          resource_key="name"
          payload='{"name":"demo_analytics_profile","capturedTrafficExternalLogging":"disabled","capturedTrafficInternalLogging":"disabled","collectGeo":"enabled","collectIp":"enabled","collectMaxTpsAndThroughput":"enabled","collectMethods":"enabled","collectPageLoadTime":"enabled","collectResponseCodes":"enabled","collectSubnets":"enabled","collectUrl":"enabled","collectUserAgent":"enabled","collectUserSessions":"enabled","collectedStatsExternalLogging":"disabled","collectedStatsInternalLogging":"enabled","defaultsFrom":"/Common/analytics","notificationByEmail":"disabled","notificationBySnmp":"disabled","notificationBySyslog":"disabled","partition":"Common","publishIruleStatistics":"disabled","sampling":"enabled","sessionCookieSecurity":"ssl-only","sessionTimeoutMinutes":"5"}'
      ignore_errors: True
    #TODO: REST call for datagroup not idempotent
    #ex.
    #failed: [52.26.61.0 -> localhost] => {"failed": true, "name": "mgmt/tm/ltm/data-group/internal", "rc": 1}
    #msg: 400 Client Error: Bad Request. {u'errorStack': [], u'message': u'"type" may not be specified in the context of the "modify" command. "type" may be specified using the following commands: create, edit, list', u'code': 400}

    - name: Uploading Datagroup ... background for sorry page
      delegate_to: localhost
      bigip_config:
          state=present
          host={{ ansible_ssh_host }}
          user={{ bigip_rest_user }}
          password={{ bigip_rest_password }}
          collection_path='mgmt/tm/ltm/data-group/internal'
          resource_key="name"
          payload='{"name":"background_image","type":"string","records":[{"name":"{{background_image}}"}]}'
      ignore_errors: True

    - name: Uploading Datagroup ... image for sorry page
      delegate_to: localhost
      bigip_config:
          state=present
          host={{ ansible_ssh_host }}
          user={{ bigip_rest_user }}
          password={{ bigip_rest_password }}
          collection_path='mgmt/tm/ltm/data-group/internal'
          resource_key="name"
          payload='{"name":"sorry_images","type":"string","records":[{"name":"{{sorry_image}}"}]}'
      ignore_errors: True
    
    - name: Uploading iRules ... sorry_page_rule
      delegate_to: localhost
      bigip_config:
          state=present
          host={{ ansible_ssh_host }}
          user={{ bigip_rest_user }}
          password={{ bigip_rest_password }}
          collection_path='mgmt/tm/ltm/rule'
          resource_key="name"
          payload='{"name":"__sorry_page_rule","apiAnonymous":"{{sorry_page_rule|replace("\\","\\\\")|replace("\"","\\\"")|replace("\n","\\n")}}"}'
    
    - name: Uploading iRules ... demo_analytics_rule
      delegate_to: localhost
      bigip_config:
          state=present
          host={{ ansible_ssh_host }}
          user={{ bigip_rest_user }}
          password={{ bigip_rest_password }}
          collection_path='mgmt/tm/ltm/rule'
          resource_key="name"
          payload='{"name":"__demo_analytics_rule","apiAnonymous":"{{demo_analytics_rule|replace("\\","\\\\")|replace("\"","\\\"")|replace("\n","\\n")}}"}'

    - name: Deploying/updating iApp
      delegate_to: localhost
      bigip_config:
          state=present
          host={{ ansible_ssh_host }}
          user={{ bigip_rest_user }}
          password={{ bigip_rest_password }}
          payload='{{demo_iApp|to_nice_json}}'
          collection_path='mgmt/tm/sys/application/service'
          resource_key="name"
      ignore_errors: true

# Finally attach an EIP to the VIP / Application. One per cluster
- hosts: bigip-cluster-seeds
  gather_facts: no
  vars:
     ansible_connection: local
     ansible_python_interpreter: "/usr/bin/env python"
     vip_id: "Vip1"
  #Grab Interface IDs and Secondary IPs from each BIGIP
  vars_files:
     - "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}.yml"
  tasks:
    - name: deploy eips
      include: "{{ install_path }}/roles/infra/tasks/deploy_eip_cft.yml"
      delegate_to: localhost
