---
# Unfortunately, add_hosts does not persist across plays so we have to rebuild the 
# topology from scratch
# Here we solve this problem by looping through results outputed from previous
# provisioning steps
# The other alternative was to use the VARs YAML output from previous provisioning 
# steps but ansible doesn't seem to load them sequentially on a task-by-task basis and 
# seems to try to preload the var files for all the tasks in the playbook causing caching issues 
# (ie. loads output previous playbook runs instead of newer desired output from an earlier task)
# cat via shell ugly but at least gives task-by-task control
# FUTURE: try fact caching.

# - hosts: bigip-managers
#   gather_facts: no
#   vars_files:
#      - "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}.yml"
#   tasks:
# #    - shell: "cat ~/vars/f5aws/env/{{ env_name }}/{{ item }}.json"
# #      register: output
# #      with_items: groups['bigip-managers']
# 
#     - add_host: name="{{ stack_outputs.ManagementInterfacePublicIp }}" group=bigips
#         ExternalInterfacePublicIp="{{ stack_outputs.ExternalInterfacePublicIp }}"
#         ExternalInterfacePrivateIp="{{ stack_outputs.ExternalInterfacePrivateIp }}"
#         InternalInterfacePrivateIp="{{ stack_outputs.InternalInterfacePrivateIp }}"
# 
# 
# - hosts: apphost-managers
#   gather_facts: no
#   vars_files:
#     - "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}.yml"
#   tasks:
#     - add_host: name="{{ stack_outputs.WebServerInstancePublicIp }}" group=apphosts
#         WebServerInstancePublicIp="{{ stack_outputs.WebServerInstancePublicIp }}"
#         WebServerInstancePrivateIp="{{ stack_outputs.WebServerInstancePrivateIp }}"
#         ansible_ssh_user="ec2-user"
# 

# Re-Create a bigip temp group from data persisted to disk
- hosts: localhost
  vars:
    vip_id: "Vip1"
  gather_facts: no
  tasks:
    - shell: "cat ~/vars/f5aws/env/{{ env_name }}/{{ item }}.json"
      register: output
      with_items: groups['bigip-managers']

    - add_host: group=bigips name="{{ item.stdout | from_json | attr('get')('ManagementInterfacePublicIp') }}"
        ExternalInterfacePublicIp="{{ item.stdout | from_json | attr('get')('ExternalInterfacePublicIp') }}"
        ExternalInterfacePrivateIp="{{ item.stdout | from_json | attr('get')('ExternalInterfacePrivateIp') }}"
        InternalInterfacePrivateIp="{{ item.stdout | from_json | attr('get')('InternalInterfacePrivateIp') }}"
        VipAddress="{{ item.stdout | from_json | attr('get')('Vip1') }}"  
        ansible_ssh_user="admin" 
      with_items: output['results']

    # Re-Create a docker host temp group from data persisted to disk
    - shell: "cat ~/vars/f5aws/env/{{ env_name }}/{{ item }}.json"
      register: output
      with_items: groups['apphost-managers']

    - add_host: name="{{ item.stdout | from_json | attr('get')('WebServerInstancePublicIp') }}" group=apphosts
        WebServerInstancePublicIp="{{ item.stdout | from_json | attr('get')('WebServerInstancePublicIp') }}"
        WebServerInstancePrivateIp="{{ item.stdout | from_json | attr('get')('WebServerInstancePrivateIp') }}"
        ansible_ssh_user="ec2-user"
      with_items: output['results']

# Hmm, what is best way to get the docker images loaded.. 
# Can use naming convention for now.
# Basically using this to pickup all the docker_containers variable data
- hosts: apphosts
  gather_facts: no
  vars_files:
    - "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}_docker_containers.yml"
  tasks:
    - debug: var=apphosts


## Now that topology and variables are loaded again
## Deploy App Stuff Here....
# 
- hosts: apphosts
  gather_facts: no
  vars:
    vip_id: "Vip1"
  tasks:
#
    - name: Store pool members from containers into json
      local_action: template src=../roles/bigip_app/templates/bigip_pool_members_from_containers_test.cfg.j2 dest=~/vars/f5aws/env/{{ env_name }}/{{vip_id}}_pool_from_containers.json

     # Notes:
     # strip out last comma from jinja template output (see note below)
     # json.loads is more sensitive than our parser and need to strip out last "," at the end of the pool member list
     # no clean way to use loop.last directive in jinja template because it's a nested loop over all docker hosts
     # Could try a inline replacement
     # json_payload: "{{json_output|regex_replace(',]}', ']}' ) }}"
     # easier for now to modify in place with ansible replace command than jinja regex filter

    - replace: dest=~/vars/f5aws/env/{{ env_name }}/{{vip_id}}_pool_from_containers.json regexp=',]}' replace=']}'
      delegate_to: localhost

- hosts: bigips
  gather_facts: no
  vars:
    vip_id: "Vip1"

    # Ansible modules (lookup/cat/etc.) auto converts file contents that contain json format into python dicts
    # so have to remove new lines and convert back to json ( with to_nice_json filter) so json modules don't fail
    # ex.
    # json/decoder.py", line 381, in raw_decode
    pool_json_payload: "{{ lookup('file', '~/vars/f5aws/env/' + env_name + '/' + vip_id + '_pool_from_containers.json')|replace('\n','') }}"
  tasks:

    - name: Setup a Webserver Pool
      delegate_to: localhost
      bigip_config:
          state=present
          host="{{ inventory_hostname }}"
          user="{{ bigip_rest_user }}"
          password="{{ bigip_rest_pass }}"
          collection_path='mgmt/tm/ltm/pool'
          resource_key="name"
          payload='{{pool_json_payload|to_nice_json}}'

    - name: Setup a High Speed Logging pool to send to Analytics Server
      delegate_to: localhost
      bigip_config:
          state=present
          host="{{ inventory_hostname }}"
          user="{{ bigip_rest_user }}"
          password="{{ bigip_rest_pass }}"
          collection_path='mgmt/tm/ltm/pool'
          resource_key="name"
          payload='{"name":"syslog_pool","members":[{"name":"10.0.3.32:514","address":"10.0.3.32"},{"name":"10.0.3.33:514","address":"10.0.3.33"}],"monitor":"tcp"}'

    #Deploy HTTP VIP
    - name: Setup the Virtual
      delegate_to: localhost
      bigip_config:
          state=present
          host="{{ inventory_hostname }}"
          user="{{ bigip_rest_user }}"
          password="{{ bigip_rest_pass }}"
          payload='{"name":"{{vip_id}}","destination":"/Common/{{VipAddress}}:80","mask":"255.255.255.255","ipProtocol":"tcp","pool":"/Common/{{vip_id}}_pool","translateAddress":"enabled","translatePort":"enabled","sourceAddressTranslation":{"type":"automap"},"profiles":[{"name":"http"},{"name":"tcp-wan-optimized","context":"clientside"},{"name":"tcp-lan-optimized","context":"serverside"}]}'
          collection_path='mgmt/tm/ltm/virtual'
          resource_key="name"

    #Deploy HTTP VIP
    - name: Setup the HTTPS Virtual
      delegate_to: localhost
      bigip_config:
          state=present
          host="{{ inventory_hostname }}"
          user="{{ bigip_rest_user }}"
          password="{{ bigip_rest_pass }}"
          collection_path='mgmt/tm/ltm/virtual'
          resource_key="name"
          payload='{"name":"{{vip_id}}_SSL_Offload","destination":"/Common/{{VipAddress}}:443","mask":"255.255.255.255","ipProtocol":"tcp","pool":"/Common/{{vip_id}}_pool","translateAddress":"enabled","translatePort":"enabled","sourceAddressTranslation":{"type":"automap"},"rules":["/Common/__snat_random_rule"], "profiles":[{"name":"http"},{"name":"tcp-ssl-wan-optimized","context":"clientside"},{"name":"tcp-ssl-lan-optimized","context":"serverside"},{"name":"clientssl","context":"clientside"}]}'

# Finally attach an EIP to the Vip / Application
- hosts: bigip-managers
  gather_facts: no
  vars:
    vip_id: "Vip1"
  #Grab Interface IDs and Secondary IPs from each BIGIP
  vars_files:
     - "~/vars/f5aws/env/{{ env_name }}/{{ inventory_hostname }}.yml"
  tasks:
    - name: deploy eips
      include: "{{ install_path }}/roles/bigip_app/tasks/deploy_eip.yml"


